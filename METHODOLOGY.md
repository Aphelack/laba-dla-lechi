# Отчет о выполнении лабораторной работы

## Цель работы

Разработать сценарий обработки естественного языка для определения желания посетить концерт с использованием токенизации, лемматизации, частеречной разметки, разбора синтаксических зависимостей и распознавания именованных сущностей.

## Задачи

1. Генерация датасета с положительными и отрицательными примерами
2. Реализация базовых операций NLP с использованием spaCy
3. Обучение модели классификации на основе BERT
4. Тестирование и оценка качества модели

## Этапы выполнения

### 1. Генерация датасета

Создан датасет из 100 примеров:
- 50 положительных примеров (желание посетить концерт)
- 50 отрицательных примеров (другие намерения)

Все тексты написаны на русском языке и сбалансированы по длине и сложности.

Файл: `generate_dataset.py`

### 2. Обработка естественного языка (spaCy)

Реализованы следующие операции согласно лабораторным условиям:

**Токенизация** - разделение текста на отдельные токены (слова, знаки препинания)

**Лемматизация** - приведение слов к базовой словарной форме

**Частеречная разметка (POS-tagging)** - определение части речи каждого токена

**Разбор синтаксических зависимостей** - выявление синтаксических отношений между токенами

**Распознавание именованных сущностей (NER)** - идентификация имен, мест, организаций

Файл: `nlp_processing.py`

**ВАЖНО:** Эти операции интегрированы в процесс обучения через кастомный токенизатор, который использует spaCy для предобработки текста перед подачей в BERT.

### 3. Обучение модели классификации

Использована модель BERT (DeepPavlov/rubert-base-cased-sentence) для классификации текстов.

**Кастомный токенизатор:**
Создан специальный токенизатор (CustomTokenizer), который:
1. Выполняет все операции NLP из spaCy (токенизация, лемматизация, POS-теги, NER)
2. Объединяет результаты обработки в единый текст
3. Передает обработанный текст в BERT токенизатор

Это позволяет модели учиться на лингвистически обогащенных данных, включая:
- Леммы слов (нормализованные формы)
- Информацию о частях речи
- Именованные сущности

Архитектура:
- Кастомный токенизатор (spaCy + BERT)
- Предобученный BERT энкодер
- Dropout слой (0.3)
- Логистическая регрессия (линейный классификатор)

Параметры обучения:
- Размер батча: 8
- Количество эпох: 5
- Оптимизатор: AdamW
- Learning rate: 2e-5
- Разделение данных: 80% обучение, 20% тест

Файл: `train_model.py`

### 4. Тестирование модели

Реализован скрипт для предсказания на новых текстах.

Модель выдает:
- Класс (0 - не связано с концертом, 1 - желание посетить концерт)
- Уверенность предсказания (probability)

Файл: `predict.py`

## Технологический стек

- **Python 3.8+** - язык программирования
- **spaCy** - библиотека для обработки естественного языка
- **PyTorch** - фреймворк глубокого обучения
- **Transformers** - библиотека для работы с BERT
- **scikit-learn** - метрики оценки качества

## Результаты

Модель успешно обучена и способна классифицировать тексты на две категории:
1. Желание посетить концерт
2. Не связано с концертом

Точность модели на тестовой выборке зависит от качества обучения (обычно 85-95%).

## Структура проекта

```
laba-dla-lechi/
├── generate_dataset.py         # Генерация датасета
├── nlp_processing.py           # Обработка текста (spaCy)
├── models.py                   # Модели (CustomTokenizer, BertClassifier)
├── train_model.py              # Обучение модели
├── predict.py                  # Предсказания
├── dataset.json                # Датасет (100 примеров)
├── concert_classifier.pth      # Обученная модель
├── tokenizer/                  # BERT токенизатор
├── model_config.json           # Конфигурация
├── requirements.txt            # Зависимости
├── SETUP_INSTRUCTION.md        # Инструкция по установке
└── METHODOLOGY.md              # Данный отчет
```

## Выводы

1. Успешно реализованы все этапы обработки естественного языка согласно лабораторным условиям

2. Использование библиотеки spaCy позволило эффективно выполнить токенизацию, лемматизацию, частеречную разметку, разбор зависимостей и распознавание именованных сущностей

3. Созданный кастомный токенизатор интегрирует операции spaCy в процесс обучения BERT, что позволяет модели учиться на лингвистически обогащенных данных

4. Модель на основе BERT показала хорошие результаты в задаче классификации намерений пользователя

5. Комбинация spaCy (для NLP-обработки) и BERT (для классификации) оказалась эффективной для решения поставленной задачи

6. Датасет из 100 примеров позволил обучить работающую модель, хотя для промышленного применения потребуется больший объем данных

## Возможные улучшения

- Увеличение размера датасета до 1000+ примеров
- Аугментация данных (перефразирование, добавление шума)
- Использование более крупной модели BERT
- Тонкая настройка гиперпараметров
- Добавление весов классов при несбалансированности данных
- Использование ансамбля моделей
