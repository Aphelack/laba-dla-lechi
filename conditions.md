Учреждение образования «Белорусский государственный университет 
информатики и радиоэлектроники» 
 
 
 
 
 
Кафедра микро- и наноэлектроники 
 
Научно-исследовательская лаборатория 4.4 
«Компьютерное проектирование микро-  
и наноэлектронных систем» 
 
 
 
Методы и программное обеспечение обработки информации  
для студентов специальности 
1-41 01 04 «Нанотехнологии и наноматериалы в электронике» 
 
Лабораторная работа № 1 
Инструменты для работы с текстовыми данными 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 2 
 
 
1 Алгоритм и ход выполнения лабораторной работы  
 
Цель: сформировать знания об основных аспектах обработки 
естественного языка (ОЕЯ) и анализа информации, включая теоретические, 
методологические и практические вопросы. 
Для достижения поставленной цели необходимо решить следующие 
задачи: 
1 Изучить теоретический материал по основам концепции и задачам 
ОЕЯ.  
2 Ознакомиться с инструментами и библиотеками для ОЕЯ (NLTK, 
spaCy, Transformers и др.).  
3 Изучить 
методы 
анализа 
текста 
(токенизация, 
лемматизация, 
извлечение именованных сущностей, определение тональности).  
4 Применить 
ОЕЯ 
в 
задачах 
автоматического 
реферирования, 
классификации текста, машинного перевода в соответствии с вариантом. 
5 Обработать полученные результаты. 
6 Выполнить анализ и сформулировать выводы по результатам 
выполнения лабораторной работы. 
Варианты заданий: 
Разработать сценарий, содержащий токенизацию, лемматизацию, 
частеречную разметку, разбор синтаксических зависимостей и распознавание 
именованных 
сущностей 
текста, 
который 
позволяет 
определить 
в 
соответствии с вариантом (таблица 1.1). 
 
Таблица 1.1 – Варианты заданий 
Вариант 
Задание 
1 
Желание приобрести билет на поезд 
2 
Желание приобрести электронное устройство 
3 
Желание посетить спектакль 
4 
Желание посетить музей 
5 
Желание посетить концерт 
6 
Полезность научной публикации 
7 
Эмоциональную окраску текста (комментария к товару) 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 3 
 
2 Теоретическая часть 
 
Обработка естественного языка – область искусственного интеллекта, 
которая занимается взаимодействием между компьютерами и человеческим 
языком. Основная цель ОЕЯ – позволить компьютерам понимать, 
интерпретировать и генерировать человеческий язык таким образом, чтобы 
это было полезно для различных приложений. ОЕЯ используется в различных 
областях, таких как чат-боты, переводчики, системы анализа тональности и 
многое другое. 
ОЕЯ играет ключевую роль в том, как мы взаимодействуем с 
технологиями. Представьте себе, что вы можете разговаривать с вашим 
компьютером или смартфоном так же, как с другом. Это стало возможным 
благодаря достижениям в области ОЕЯ. Например, когда вы используете 
голосового помощника, такого как Siri или Google Assistant, вы 
взаимодействуете с системой, которая использует ОЕЯ для понимания и 
обработки ваших запросов. 
Основными задачами ОЕЯ является: 
1 Токенизация: Разделение текста на отдельные слова или предложения. 
Это первый шаг в большинстве задач ОЕЯ. Например, предложение "Я люблю 
программирование" будет разделено на три токена: "Я", "люблю" и 
"программирование". 
2 Лемматизация и стемминг: Приведение слов к их базовой или 
корневой форме. Лемматизация учитывает контекст и преобразует слова в их 
базовую форму, тогда как стемминг просто удаляет окончания. Например, 
слова "бегу", "бегал" и "бегать" будут приведены к одной форме "бег". 
3 Частеречная разметка (POS-tagging): Определение частей речи для 
каждого слова в предложении. Это помогает понять грамматическую 
структуру текста. Например, в предложении "Кошка сидит на ковре" слова 
будут размечены как существительное, глагол и предлог. 
4 Распознавание именованных сущностей (NER): Идентификация и 
классификация именованных сущностей в тексте (например, имена людей, 
названия мест). Например, в предложении "Иван живет в Москве" система 
распознает "Иван" как имя человека, а "Москва" как название города. 
5 Анализ тональности: Определение эмоциональной окраски текста 
(положительная, отрицательная, нейтральная). Это важно для анализа отзывов 
и комментариев. Например, отзыв "Этот продукт отличный!" будет 
классифицирован как положительный. 
6 Синтаксический анализ: Построение синтаксического дерева для 
предложения. Это помогает понять грамматическую структуру и отношения 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 4 
 
между словами. Например, в предложении "Кошка сидит на ковре" 
синтаксический анализ покажет, что "сидит" – это глагол, связанный с 
подлежащим "Кошка". 
7 Машинный перевод: Перевод текста с одного языка на другой. 
Современные системы машинного перевода, такие как Google Translate, могут 
учитывать контекст и обеспечивать более точные переводы. 
8 Распознавание речи: Преобразование аудиозаписей речи в текст. Это 
полезно для создания транскрипций и голосовых команд. Например, система 
распознавания речи может преобразовать аудиозапись лекции в текстовый 
документ. 
К методам ОЕЯ относят: 
1 Правила и шаблоны: Использование заранее определенных правил для 
обработки текста. Этот метод был одним из первых в ОЕЯ и до сих пор 
используется для простых задач. Например, можно создать правило, которое 
будет распознавать все даты в тексте. 
2 Статистические методы: Применение вероятностных моделей, таких 
как наивный байесовский классификатор. Эти методы основаны на 
статистическом анализе больших объемов данных. Например, наивный 
байесовский классификатор может использоваться для классификации писем 
как спам или не спам. 
3 Машинное обучение: Использование алгоритмов машинного обучения 
для обучения моделей на больших объемах данных. Эти методы позволяют 
создавать более сложные и точные модели. Например, алгоритмы машинного 
обучения могут использоваться для предсказания следующего слова в 
предложении. 
4 Глубокое обучение: Применение нейронных сетей, таких как 
рекуррентные нейронные сети (RNN) и трансформеры. Эти методы позволяют 
создавать модели, которые могут учитывать контекст и отношения между 
словами. Например, модели на основе трансформеров, такие как BERT и GPT-
3, могут использоваться для генерации текста и машинного перевода. 
К популярным инструментам и библиотекам для ОЕЯ можно отнести: 
1 NLTK (Natural Language Toolkit) – это одна из самых популярных 
библиотек для обработки естественного языка на Python. Она предоставляет 
множество инструментов для токенизации, лемматизации, POS-теггинга и 
многого другого. NLTK также включает в себя множество учебных 
материалов и примеров, что делает ее отличным выбором для новичков. 
Например, с помощью NLTK можно легко создать программу, которая будет 
анализировать тональность отзывов на продукты. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 5 
 
2 SpaCy – это мощная библиотека для ОЕЯ, которая отличается высокой 
производительностью и простотой использования. Она поддерживает 
множество языков и предоставляет инструменты для токенизации, NER, POS-
теггинга и синтаксического анализа. SpaCy также интегрируется с другими 
библиотеками, такими как TensorFlow и PyTorch, что позволяет использовать 
ее в более сложных проектах. Например, с помощью SpaCy можно создать 
систему, которая будет автоматически классифицировать документы по 
темам. 
3 Transformers (от Hugging Face) – это библиотека, разработанная 
компанией Hugging Face, которая предоставляет доступ к передовым моделям 
глубокого обучения для ОЕЯ, таким как BERT, GPT-3 и другие. Эти модели 
могут использоваться для различных задач, включая машинный перевод, 
анализ тональности и генерацию текста. Например, с помощью модели GPT-3 
можно создать чат-бота, который будет вести осмысленные беседы с 
пользователями. 
4 Gensim – это библиотека для тематического моделирования и 
обработки больших текстовых корпусов. Она особенно полезна для задач, 
связанных с семантическим анализом и моделированием тем. Например, с 
помощью Gensim можно анализировать большие объемы текстов и выявлять 
основные темы и тенденции. 
Расмотрим некоторые примеры применения ОЕЯ в реальных задачах: 
1 Чат-боты и виртуальные ассистенты: Чат-боты, такие как Siri, Alexa и 
Google Assistant, используют ОЕЯ для понимания и обработки запросов 
пользователей. Они могут отвечать на вопросы, выполнять команды и даже 
вести беседы. Например, вы можете попросить Siri установить будильник или 
узнать погоду, и она выполнит вашу команду, используя ОЕЯ для понимания 
вашего запроса. 
2 Анализ тональности в социальных сетях: Компании используют ОЕЯ 
для анализа тональности отзывов и комментариев в социальных сетях. Это 
помогает им понимать мнение клиентов и реагировать на негативные отзывы. 
Например, с помощью анализа тональности можно выявить, какие аспекты 
продукта вызывают наибольшее недовольство у пользователей и принять 
меры для их улучшения. 
3 Машинный перевод: Сервисы, такие как Google Translate, используют 
ОЕЯ для перевода текста с одного языка на другой. Современные модели 
машинного перевода могут учитывать контекст и обеспечивать более точные 
переводы. Например, Google Translate может переводить целые предложения 
и абзацы, сохраняя их смысл и грамматическую структуру. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 6 
 
4 Автоматическое суммирование текста: ОЕЯ используется для 
создания кратких резюме длинных текстов. Это полезно для новостных 
агрегаторов, научных статей и других приложений, где важно быстро 
получить основную информацию. Например, система автоматического 
суммирования может создать краткое резюме новостной статьи, выделив 
основные события и факты. 
5 Распознавание речи: Системы распознавания речи, такие как Dragon 
NaturallySpeaking, используют ОЕЯ для преобразования аудиозаписей речи в 
текст. Это полезно для создания транскрипций, голосовых команд и других 
приложений. Например, система распознавания речи может использоваться 
для создания текстовых версий лекций и интервью. 
 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 7 
 
3 Практическая часть 
 
При выполнении работы используется spaCy – популярная библиотека 
Python, содержащая все лингвистические данные и алгоритмы, необходимые 
для обработки текстов на естественном языке. SpaCy крайне проста в работе 
благодаря объектам-контейнерам, которые соответствуют элементам текста 
на естественном языке, например предложениям и словам. У этих объектов, в 
свою очередь, есть атрибуты, соответствующие лингвистическим признакам, 
– например, принадлежность к той или иной части речи. На момент написания 
книги в spaCy были включены предобученные модели для английского, 
немецкого, греческого, испанского, французского, итальянского, литовского, 
норвежского стандарта букмол, нидерландского и португальского языков, а 
также многоязыковая модель (на GitHub можно найти неофициальные модели 
и для русского языка). Кроме того, в spaCy есть встроенные средства 
визуализации, позволяющие генерировать наглядное представление о 
синтаксической структуре предложений или об именованных сущностях 
документа. 
Библиотека spaCy предлагает нативную поддержку продвинутых 
возможностей ОЕЯ, отсутствующую в других популярных библиотеках ОЕЯ 
для языка Python. Например, spaCy, в отличие от пакета Natural Language 
Toolkit (NLTK), может похвастаться нативной поддержкой векторов слов. При 
использовании NLTK пришлось бы обратиться к сторонней утилите 
наподобие Gensim – реализации алгоритма word2vec для языка Python. 
При работе со spaCy можно настроить под себя уже существующие 
модели или отдельные компоненты моделей, обучить собственные модели с 
нуля в соответствии с потребностями своих приложений, а также подключить 
статистические модели, обученные с помощью других популярных библиотек 
машинного обучения: TensorFlow, Keras, scikit-learn и PyTorch. Кроме того, 
spaCy без проблем может взаимодействовать с другими библиотеками 
экосистемы ИИ языка Python, позволяя, например, использовать для чат-бота 
возможности машинного зрения. 
 
3.1 Настройка рабочей среды 
Прежде чем приступить к использованию spaCy, необходимо настроить 
рабочую среду, установив на машине следующие программные компоненты: 
– Python 2.7 (или более позднюю версию) либо Python 3.4 (или более 
позднюю версию); 
– библиотеку spaCy; 
– статистическую модель для spaCy. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 8 
 
Скачать Python можно по ссылке https://www.python.org/downloads/ и 
далее выполнить инструкции по настройке среды Python. Установите spaCy в 
среде Python с помощью pip, введя команду: 
$ pip install spacy 
Если в вашей системе установлено несколько версий Python, выберите 
pip, связанный с той версией, с которой вы хотели бы работать. Например, для 
использования spaCy с Python 3.5 выполните команду: 
$ pip3.5 install spacy 
Если библиотека spaCy уже установлена в вашей системе, можете 
обновить ее до текущей версии. Проверить, какая версия spaCy установлена у 
вас, можно с помощью команды: 
$ python -m spacy info 
Опять же необходимо заменить команду python на команду 
исполняемого файла той версии Python, которая используется в вашей 
конкретной среде. Здесь и далее будут указаны просто python и pip. 
Для обновления до последней версии установленного в системе пакета 
spaCy можно воспользоваться следующей командой pip: 
$ pip install -U spacy 
 
3.2 Установка статистических моделей для библиотеки spaCy 
В установочные пакеты spaCy не включены статистические модели, 
необходимые при использовании библиотеки. Статистические модели 
содержат знания о конкретном языке, собранные из множества источников. 
Нужную вам модель придется отдельно скачать и установить. 
Доступно несколько предобученных статистических моделей для 
различных языков. С сайта spaCy вы можете скачать, например, такие модели 
для английского языка: en_core_web_sm, en_core_web_md, en_core_web_lg и 
en_vectors_web_lg. Названия моделей создаются по следующему принципу: 
lang_type_genre_size. Lang обозначает язык. Type указывает на возможности 
модели (например, core означает, что речь идет об универсальной модели, 
имеющей словарь, синтаксис, сущности и векторы). Genre указывает на тип 
текстов, которые лежали в основе обучения данной модели: web («Википедия» 
или подобные ресурсы) или news (новостные статьи). Size обозначает размер 
модели: lg – большая, md – средняя и sm – маленькая. Чем больше модель, тем 
больше 
дискового 
пространства 
ей 
нужно. 
Например, 
модель 
en_vectors_web_lg-2.1.0 занимает 631 Мбайт, в то время как en_core_web_sm-
2.1.0 – лишь 10 Мбайт. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 9 
 
Для скачивания конкретной модели необходимо указать ее название 
следующим образом: 
$python -m spacy download en_core_web_md 
После установки модель можно загрузить с помощью того же самого 
сокращенного названия, что и во время установки: 
nlp = spacy.load('en') 
 
3.3 Базовые операции NLP в библиотеке spaCy 
Начнем с цепочки базовых операций NLP – конвейера обработки. 
Библиотека SpaCy выполняет все эти операции неявно, поэтому мы можем 
сосредоточиться 
на 
логике 
приложения. 
На 
рисунке 3.1 
приведена 
упрощенная схема процесса. 
 
 
Рисунок 3.1 – Упрощенная схема конвейера обработки 
Конвейер 
обработки 
обычно 
включает 
операции 
токенизации, 
лемматизации, частеречной разметки, разбора синтаксических зависимостей и 
распознавания именованных сущностей. 
 
3.3.1 Токенизация 
Самое первое действие любого ОЕЯ-приложения – разбор текста на 
токены, которые могут быть словами, числами или знаками препинания. 
Токенизация – самая первая операция в конвейере, поскольку для всех 
остальных операций необходимо, чтобы текст был разобран на токены. 
Следующий код демонстрирует процесс токенизации: 
import spacy 
nlp = spacy.load('en') 
doc = nlp(u'I am flying to Frisco') 
print([w.text for w in doc]) 
Начинаем с импорта библиотеки spaCy, чтобы получить доступ к ее 
функциональности, после чего загружаем пакет модели с помощью 
сокращения en, чтобы создать экземпляр класса Language библиотеки spaCy. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 10 
 
Объект Language содержит словарь языка и другие данные статистической 
модели. Назовем его Language nlp. 
Далее применяем только что созданный объект к нашему примеру 
предложения, создавая объект Doc – контейнер для последовательности 
объектов Token. Библиотека spaCy генерирует его неявным образом на основе 
переданного ей текста. 
К этому моменту с помощью всего лишь трех строк кода нам удалось 
добиться от spaCy генерации грамматической структуры для нашего примера 
предложения. Как ее использовать – решаете вы. В данном очень простом 
примере нам удалось только вывести текстовое содержимое (text content) всех 
токенов предложения. 
Наш код выводит токены предложения в виде списка: 
['I', 'am', 'flying', 'to', 'Frisco'] 
Текстовое содержимое – это группа символов, составляющих токен. 
Например, буквы a и m в токене am – лишь одно из многих свойств объекта 
Token. 
 
3.3.2 Лемматизация 
Лемма – это базовая, фактически словарная форма токена. Например, 
лемма токена flying – fly. Лемматизация – процесс сведения словоформ к 
соответствующим леммам. В следующем коде приведен простой пример 
лемматизации с помощью библиотеки spaCy: 
import spacy 
nlp = spacy.load('en') 
doc = nlp(u'this product integrates both libraries 
for downloading 
and applying patches') 
for token in doc: 
print(token.text, token.lemma_) 
Первые три строки этого сценария такие же, как и в предыдущем 
сценарии. Получив объект Doc с токенами, проходим в цикле по этим токенам, 
после чего выводим текстовое содержимое каждого из них вместе с 
соответствующей леммой. Результат, который выводит этот сценарий, 
выглядит следующим образом: 
this 
this 
product 
product 
integrates 
integrate 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 11 
 
both 
both 
libraries 
library 
for 
for 
downloading 
download 
and 
and 
applying 
apply 
patches 
patch 
Столбец слева содержит токены, столбец справа – соответствующие 
леммы. 
При решении задачи распознавания смысла особо важное значение 
имеет этап лемматизации. Чтобы понять почему, вернемся к предложению-
примеру из предыдущего раздела: 
I am flying to Frisco. 
Пусть это предложение было подано на вход ОЭЯ-приложения, 
взаимодействующего с онлайн-системой, которая предоставляет API для 
бронирования билетов на различный транспорт. Приложение обрабатывает 
запрос пользователя, выделяет из него необходимую информацию и передает 
ее нижележащему API.  
ОЕЯ-приложение пытается извлечь из запроса пользователя следующую 
информацию: вид путешествия (авиа, поезд, автобус и т. д.) и пункт 
назначения. Сначала приложению необходимо выяснить, что хочет 
приобрести пользователь: авиабилет, билет на поезд или билет на автобус. Для 
этого оно ищет слово, совпадающее с одним из ключевых слов заранее 
заданного списка. Простейший способ облегчить поиск – преобразовать все 
слова в предложении в соответствующие леммы, и тогда список ключевых 
слов может быть более коротким и понятным. Например, нет необходимости 
включать в него все словоформы глагола fly (такие как fly, flying, flew и flown), 
чтобы приложение могло понять желание пользователя заказать именно 
авиабилет. Достаточно свести все возможные варианты к базовой форме слова 
– fly. 
Лемматизация также удобна при определении пункта назначения на 
основе запроса пользователя. У многих городов мира есть прозвища, но для 
системы бронирования билетов необходимы официальные названия. Конечно, 
объекту Tokenizer, который выполняет лемматизацию по умолчанию, 
неизвестна разница между разговорными и официальными названиями 
городов, стран и т. д. Для решения этой проблемы добавим в имеющийся 
экземпляр Tokenizer правила для подобных исключений. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 12 
 
Следующий 
сценарий 
иллюстрирует 
возможную 
реализацию 
лемматизации для нескольких городов назначения. В нем выводятся леммы 
составляющих предложение слов. 
import spacy 
from spacy.symbols import ORTH, LEMMA 
nlp = spacy.load('en') 
doc = nlp(u'I am flying to Frisco') 
print([w.text for w in doc])special_case = [{ORTH: 
u'Frisco', LEMMA: u'San 
Francisco'}]nlp.tokenizer.add_special_case(u'Frisco
', special_case)print([w.lemma_ for w in nlp(u'I am 
flying to Frisco')]) 
Описываем исключение (special case) для слова Frisco, заменив его 
лемму по умолчанию на San Francisco. Далее добавляем это исключение в 
экземпляр Tokenizer: теперь Tokenizer всякий раз будет использовать данное 
исключение при запросе леммы для Frisco. Для проверки выводим леммы всех 
слов в предложении. В результате выполнения сценария получится 
следующее: 
['I', 'am', 'flying', 'to', 'Frisco'] 
['-PRON-', 'be', 'fly', 'to', 'San Francisco'] 
Как видим, выведены леммы всех слов заданного предложения, за 
исключением Frisco, вместо которого выводится San Francisco. 
 
3.3.3 Частеречная разметка 
Тег части речи (part-of-speech tag) указывает, к какой части речи в этом 
конкретном предложении относится конкретное слово (оно может быть 
существительным, глаголом, наречием и т. д.). 
В библиотеке spaCy теги частей речи нередко содержат и подробную 
информацию о токене. Например, в информации о глаголе могут быть указаны 
следующие признаки: время – прошедшее, настоящее или будущее; вид 
(аспект) – простой, длительный или совершенный; лицо – 1-е, 2-е или 3-е; 
число – единственное или множественное. 
Извлечение тегов частей речи для глаголов может помочь с 
определением намерения пользователя, когда токенизации и лемматизации 
недостаточно. 
Скажем, 
сценарий 
лемматизации 
для 
приложения 
бронирования билетов из предыдущего раздела не поможет ОЕЯ-приложению 
выбрать слова в предложении для составления запроса к нижележащему API: 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 13 
 
на практике подобная задача бывает достаточно сложной. Например, если 
запрос пользователя состоит более чем из одного предложения: 
I have flown to LA. Now I am flying to Frisco. 
Результат лемматизации этих предложений выглядит следующим 
образом: 
['-PRON-', 'have', 'fly', 'to', 'LA', '.', 'now', 
'-PRON-', 'be', 'fly', 'to', 'San Francisco', '.'] 
Одной лемматизации здесь недостаточно, ведь приложение может 
счесть ключевыми словами леммы fly и LA из первого предложения, 
указывающие на желание пользователя полететь в Лос-Анджелес, хотя на 
самом деле тот намерен отправиться в Сан-Франциско. Проблему усложняет 
тот факт, что при лемматизации глаголы сводятся к формам инфинитива, из-
за чего сложно определить их роль в предложении. 
В такой ситуации на помощь приходят теги частей речи. В английском 
языке в число основных частей речи входят существительное, местоимение, 
определитель, прилагательное, глагол, наречие, числительное, служебные 
части речи: предлог, союз, артикль и междометие (больше информации вы 
найдете в кратком грамматическом справочнике в приложении). В библиотеке 
spaCy представлены те же категории, а также еще несколько для обозначения 
укрупненных частей речи (coarse-grained parts of speech) – символов, знаков 
препинания и пр. Все категории доступны в виде фиксированного набора тегов 
через атрибуты Token.pos (тип int) и Token.pos_ (тип unicode). 
Кроме того, spaCy предоставляет теги для уточненных частей речи (fine-
grained parts of speech) с более подробной информацией о токене, где указаны 
морфологические признаки, времена глаголов и типы (разряды) местоимений. 
Естественно, в списке уточненных частей речи тегов гораздо больше, чем в 
списке укрупненных. Теги уточненных частей речи доступны в атрибутах 
Token.tag (тип int) и Token.tag_ (тип unicode). 
Вероятно, самые интересные для ОЕЯ-приложений свойства глаголов – 
время и вид. Взятые вместе, они указывают на то, как определенное глаголом 
действие протекает во времени. Например, форма длительного вида 
настоящего времени (present tense progressive aspect) описывает происходящее 
прямо сейчас или ожидаемое в самом ближайшем будущем. Форма 
длительного вида настоящего времени формируется путем добавления формы 
настоящего времени глагола to be перед требуемым глаголом с окончанием -
ing. Например, в предложении I am looking into it перед глаголом looking стоит 
am – глагол to be в форме первого лица настоящего времени. В этом примере 
am указывает на настоящее время, а looking – на длительный вид. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 14 
 
Приложение по бронированию билетов может использовать теги частей 
речи из библиотеки spaCy, чтобы отфильтровать глаголы в тексте и оставить 
лишь те, которые нужны для определения намерений пользователя. 
Прежде чем перейти к реализации кода этого процесса, давайте 
подумаем, какими фразами пользователи могут выражать намерение 
забронировать билет, скажем, в Лос-Анджелес. Начнем с нескольких фраз, 
содержащих следующее сочетание лемм: fly, to и LA. Вот несколько 
простейших вариантов: 
I flew to LA. 
I have flown to LA. 
I need to fly to LA. 
I am flying to LA. 
I will fly to LA. 
Обратите внимание: несмотря на то что после сведения до лемм все эти 
предложения будут включать сочетание fly to LA, лишь часть из них выражает 
реальное намерение пользователя забронировать билет в Лос-Анджелес. 
Первые два явно не подходят. 
Проанализировав ситуацию, мы поймем, что формы глагола fly простого 
прошедшего времени и прошедшего совершенного времени, использованные 
в первых двух предложениях, не подразумевают искомое намерение. Нам 
подходят только формы инфинитива и настоящего длительного времени. 
Следующий сценарий иллюстрирует, как можно найти эти формы в нашем 
примере текста: 
import spacy 
nlp = spacy.load('en') 
doc = nlp(u'I have flown to LA. Now I am flying to 
Frisco.') 
print([w.text for w in doc if w.tag_== 'VBG' or 
w.tag_== 'VB']) 
Свойство tag_ объекта Token содержит атрибут уточненной части речи 
этого объекта. Чтобы проверить, был ли токену присвоен атрибут VB (глагол 
в неопределенной, то есть инфинитивной, форме) или VBG (глагол в форме 
настоящего длительного времени), проходим в цикле по всем токенам, 
составляющим текст. 
В нашем примере текста указанным условиям удовлетворяет лишь 
глагол flying из второй фразы, поэтому результат будет следующим: 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 15 
 
['flying'] 
Конечно, теги уточненных частей речи присваиваются не только 
глаголам. Например, spaCy распознает, что LA и Frisco – имена собственные 
(то есть имена людей или названия мест, объектов и организаций), и 
присваивает им теги PROPN. При желании в предыдущий сценарий можно 
было бы добавить следующую строку кода: 
print([w.text for w in doc if w.pos_ == 'PROPN']) 
В результате ее добавления будет выведен еще и такой список: 
['LA', 'Frisco'] 
Здесь перечислены имена собственные из обоих предложений нашего 
примера текста. 
Тегов уточненных частей речи не всегда достаточно для определения 
смысла высказывания – иногда необходимо учитывать и контекст. В качестве 
примера рассмотрим следующее высказывание: I am flying to LA. Здесь flying 
– глагол в форме настоящего длительного времени, и средство частеречной 
разметки соотнесет его с тегом VBG. Но, поскольку данная форма глагола 
может описывать как то, что происходит прямо сейчас, так и то, что 
произойдет в ближайшем будущем, высказывание может означать и I’m 
already in the sky, flying to LA («Я уже в небе, лечу в Лос-Анджелес»), и I’m 
going to fly to LA («Я собираюсь лететь в Лос-Анджелес»). Но приложение 
бронирования билетов должно интерпретировать лишь одно из этих 
предложений как I need an air ticket to LA («Я хочу приобрести билет в Лос-
Анджелес»). Таким же образом рассмотрим следующий связный текст: I am 
flying to LA. In the evening, I have to be back in Frisco. Вероятнее всего, 
говорящий хочет купить авиабилет из Лос-Анджелеса в Сан-Франциско на 
вечерний рейс. 
Теперь соединим имена собственные с глаголом, который был выбран 
ранее средством частеречной разметки. Напомню, что список глаголов, 
доступных к использованию для идентификации намерения связного текста, 
включает лишь глагол flying из второго предложения. Как же теперь получить 
пару «глагол/имя собственное», лучше всего описывающую намерение 
текста? Человек, конечно, с легкостью составит пары «глагол/имя 
собственное» из слов одного и того же предложения. А поскольку глагол flown 
из первого предложения не удовлетворяет заданному условию (ему 
удовлетворяют только неопределенная форма и форма настоящего 
длительного времени), подобную пару можно составить лишь для второго 
предложения: flying, Frisco. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 16 
 
Для обработки подобных ситуаций программным образом в spaCy есть 
средство 
разбора 
синтаксических 
зависимостей, 
которое 
выявляет 
синтаксические отношения между отдельными токенами в предложении и 
соединяет дугами синтаксически связанные пары слов. 
Метки синтаксической зависимости (syntactic dependency labels), равно 
как и леммы и теги частей речи, – это лингвистические признаки, 
присваиваемые библиотекой spaCy объектам Token, которые образуют 
содержащийся в объекте Doc текст. Например, метка зависимости dobj 
соответствует прямому дополнению (direct object).  
Метка dobj присваивается слову ticket, поскольку в данном отношении 
оно играет роль дочернего элемента. В коде можно определить главный 
элемент отношения с помощью атрибута Token.head. 
Как видите, одно и то же слово в предложении может участвовать в 
нескольких синтаксических отношениях.  
Следующий 
сценарий 
иллюстрирует 
обращение 
к 
меткам 
синтаксической зависимости токенов для текста примера из подраздела 
«Частеречная разметка» на с. 47: 
import spacy 
nlp = spacy.load('en_core_web_md') 
doc = nlp(u'I have flown to LA. Now I am flying to 
Frisco.') 
for token in doc: 
print(token.text, token.pos_, token.dep_) 
Сценарий выводит теги укрупненных частей речи и метки зависимостей, 
присвоенные составляющим этот текст токенам: 
I 
PRON 
nsubj 
have 
VERB 
aux 
flown VERB 
ROOT 
to 
ADP 
prep 
LA 
PROPN pobj 
. 
PUNCT punct 
Now 
ADV 
advmod 
I 
PRON 
nsubj 
am 
VERB 
aux 
flying VERB 
ROOT 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 17 
 
to 
ADP 
prep 
Frisco PROPN pobj 
. 
PUNCT punct 
Однако выведенный список не демонстрирует связь слов в предложении 
с помощью упомянутых в начале данного раздела дуг зависимостей. Чтобы 
посмотреть дуги зависимостей для нашего примера текста, замените цикл из 
предыдущего сценария таким образом: 
for token in doc: 
print(token.head.text, token.dep_, token.text) 
Свойство head объекта Token ссылается на синтаксический главный 
элемент данного токена. Вывод этой информации позволяет увидеть, какими 
синтаксическими зависимостями связаны между собой слова в предложениях 
текста. Если изобразить зависимости графически, каждой строке следующего 
вывода будет соответствовать дуга. Исключение составляет отношение ROOT 
(поскольку слово с этой меткой – единственное в предложении, у которого нет 
главного элемента1): 
flown 
nsubj 
I 
flown 
aux 
have 
flown 
ROOT 
flown 
flown 
prep 
to 
to 
pobj 
LA 
flown 
punct 
. 
flying 
advmod 
Now 
flying 
nsubj 
I 
flying 
aux 
am 
flying 
ROOT 
flying 
flying 
prep 
to 
to 
pobj 
Frisco 
flying 
punct 
. 
Учитывая приведенный выше список синтаксических зависимостей, 
попробуем разобраться, какие метки указывают на токены, лучше всего 
описывающие намерение пользователя. Другими словами, найдем пару, 
которая способна более или менее точно передать основной посыл текста. 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 18 
 
В этом смысле многообещающе выглядят токены, маркированные 
метками зависимостей ROOT и pobj (в данном примере они играют ключевую 
роль в распознавании намерения). Как упоминалось ранее, метка ROOT 
обозначает смысловой глагол предложения, а pobj в этом примере отмечает 
сущность, которая в сочетании с глаголом резюмирует смысл всего 
высказывания. 
В следующем сценарии найдем слова, соответствующие этим двум 
меткам зависимостей: 
import spacy 
nlp = spacy.load('en') 
doc = nlp(u'I have flown to LA. Now I am flying to 
Frisco.')for sent in doc.sents: 
print([w.text for w in sent if w.dep_ == 'ROOT' or 
w.dep_ == 'pobj']) 
Здесь мы разрезаем связный текст, разбивая его на предложения с 
помощью свойства doc.sents, предназначенного для прохода в цикле по 
отдельным фразам документа. Разрезание текста на отдельные предложения 
удобно, например, для поиска определенных частей речи в каждом из 
предложений текста. (О свойстве doc.sents поговорим в следующей главе, 
когда будем рассматривать пример того, как ссылаться на токены документа с 
помощью индексов уровня предложения.) Благодаря этому на основе 
конкретных меток зависимости, присвоенных токенам, для каждого 
предложения можно создать список потенциальных ключевых слов. Условия 
фильтрации в примере выбирались исходя из анализа синтаксически 
связанных пар, сгенерированных предыдущим сценарием. В частности, 
выбирались токены с метками зависимости ROOT и pobj, поскольку именно 
они формируют интересующие нас пары. 
Результаты выполнения сценария должны выглядеть так: 
['flown', 'LA'] 
['flying', 'Frisco'] 
В обеих парах предложений выходные существительные были 
маркированы как pobj, что в приложении бронирования билетов поможет в 
выборе существительного, лучше всего подходящего к глаголу. В данном 
случае это глагол flying и существительное Frisco. 
Приведенный пример – упрощенный вариант выделения информации с 
помощью меток зависимостей. В следующих главах мы рассмотрим более 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 19 
 
сложные примеры обхода деревьев зависимостей отдельных предложений или 
даже целого текста с извлечением необходимой информации. 
 
3.3.4 Распознавание именованных сущностей 
Именованная сущность (named entity) – объект, на который можно 
ссылаться по его собственному наименованию. Именованной сущностью 
может быть человек, организация, место или другая сущность. Именованные 
сущности играют важную роль в ОЕЯ, поскольку позволяют выяснить, о 
каком месте или организации говорит пользователь. Следующий сценарий 
ищет именованные сущности в тексте, который мы использовали в 
предыдущих примерах: 
import spacy 
nlp = spacy.load('en') 
doc = nlp(u'I have flown to LA. Now I am flying to 
Frisco.') 
for token in doc: 
if token.ent_type != 0: 
print(token.text, token.ent_type_) 
Если атрибуту ent_type токена не присвоено значение 0, значит, этот 
токен – именованная сущность. В таком случае выводим атрибут ent_type_ 
токена, содержащий тип именованной сущности в поле типа unicode. В 
результате выполнения нашего сценария должно выводиться следующее: 
LA 
GPE 
Frisco 
GPE 
И LA, и Frisco маркированы меткой GPE, которая расшифровывается как 
geopolitical entity («геополитическая сущность») и обозначает страны, города, 
штаты и другие географические объекты. 
 
 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 20 
 
4 Содержание отчета 
 
Отчет о выполненной лабораторной работе должен содержать: 
1 Титульный лист. 
2 Цель и задачи лабораторной работы. 
3 Описание и анализ задания. 
4 Ход выполнения (включает описание применяемых методов и 
методик, обоснование принятых решений, снимки экрана и примеры 
применения разработанного сценария). 
5 Заключение. 
6 Приложения (листинг сценария, дополнительная информация при 
необходимости). 
 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 21 
 
5 Список источников (литература) 
 
1 Серебряная, Л. В. Модели и методы обработки данных в инфор-
мационных системах : учебно-методическое пособие / Л. В. Серебряная, В. В. 
Потараев, Е. П. Фадеева. – Минск : БГУИР, 2019. – 67 с. 
2 Осовский, С. Нейронные сети для обработки информации / С. 
Осовский ; пер. с польск. И. Д. Рудинского. – 2-е изд., перераб. и доп. – Москва 
: Горячая линия-Телеком, 2016. – 448 с. 
3 Картер, Д. Нейросети. Обработка естественного языка / Д. Картер. – 
«Автор», 2023. – 156 с. 
4 Ганегедара, Т. Обработка естественного языка с TensorFlow / пер. с 
анг. В. С. Яценкова. – М. : ДМК Пресс, 2020. – 382 с. 
5 Васильев, Ю. Обработка естественного языка. Python и spaCy на 
практике. – СПб. : Питер, 2021. – 256 с. 
6 Риз, Р. Обработка естественного языка на Java / пер. с англ. 
А.В.Снастина. – М. : ДМК Пресс, 2016. – 264 с. 
7 Хобсон, Л. Обработка естественного языка в действии / Л. Хобсон, Х. 
Ханнес, Х. Коул. – СПб. : Питер, 2020. – 576 с. 
 
 
 
 
 
 
 
 
 
 
 

Лабораторная работа №1 
Методы и программное обеспечение  
обработки информации 
Ловшенко И.Ю., Корсак К.В. 22 
 
Приложение А 
Список основных тегов частей речи 
 
ADJ – прилагательное; 
ADP – предлог; 
ADV – наречие; 
AUX – вспомогательный глагол; 
CONJ – союз; 
DET – определитель; 
INTJ – междометие; 
NOUN – существительное; 
NUM – числительное; 
PART – частица; 
PRON – местоимение; 
PROPN – собственное существительное; 
PUNCT – знак препинания; 
VERB – глагол. 
 
 
